{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6f5203",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:19.380323Z",
     "iopub.status.busy": "2021-09-27T05:33:19.379244Z",
     "iopub.status.idle": "2021-09-27T05:33:20.384376Z",
     "shell.execute_reply": "2021-09-27T05:33:20.383545Z",
     "shell.execute_reply.started": "2021-09-27T04:11:45.321206Z"
    },
    "papermill": {
     "duration": 1.032092,
     "end_time": "2021-09-27T05:33:20.384538",
     "exception": false,
     "start_time": "2021-09-27T05:33:19.352446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4842888b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.437845Z",
     "iopub.status.busy": "2021-09-27T05:33:20.435750Z",
     "iopub.status.idle": "2021-09-27T05:33:20.440967Z",
     "shell.execute_reply": "2021-09-27T05:33:20.440467Z",
     "shell.execute_reply.started": "2021-09-27T04:12:00.447958Z"
    },
    "papermill": {
     "duration": 0.034708,
     "end_time": "2021-09-27T05:33:20.441109",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.406401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469d5716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.491845Z",
     "iopub.status.busy": "2021-09-27T05:33:20.491230Z",
     "iopub.status.idle": "2021-09-27T05:33:20.493550Z",
     "shell.execute_reply": "2021-09-27T05:33:20.493002Z",
     "shell.execute_reply.started": "2021-09-27T04:12:01.089950Z"
    },
    "papermill": {
     "duration": 0.031405,
     "end_time": "2021-09-27T05:33:20.493679",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.462274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c48ab6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.557098Z",
     "iopub.status.busy": "2021-09-27T05:33:20.556442Z",
     "iopub.status.idle": "2021-09-27T05:33:20.558368Z",
     "shell.execute_reply": "2021-09-27T05:33:20.558784Z",
     "shell.execute_reply.started": "2021-09-27T04:12:01.517298Z"
    },
    "papermill": {
     "duration": 0.044004,
     "end_time": "2021-09-27T05:33:20.558968",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.514964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cee7a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.605826Z",
     "iopub.status.busy": "2021-09-27T05:33:20.605196Z",
     "iopub.status.idle": "2021-09-27T05:33:20.625588Z",
     "shell.execute_reply": "2021-09-27T05:33:20.626057Z",
     "shell.execute_reply.started": "2021-09-27T04:12:01.933410Z"
    },
    "papermill": {
     "duration": 0.046021,
     "end_time": "2021-09-27T05:33:20.626254",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.580233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d24d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.673738Z",
     "iopub.status.busy": "2021-09-27T05:33:20.673136Z",
     "iopub.status.idle": "2021-09-27T05:33:20.678743Z",
     "shell.execute_reply": "2021-09-27T05:33:20.679183Z",
     "shell.execute_reply.started": "2021-09-27T04:12:02.293501Z"
    },
    "papermill": {
     "duration": 0.030732,
     "end_time": "2021-09-27T05:33:20.679355",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.648623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = 32, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2336180f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.725960Z",
     "iopub.status.busy": "2021-09-27T05:33:20.725332Z",
     "iopub.status.idle": "2021-09-27T05:33:20.729450Z",
     "shell.execute_reply": "2021-09-27T05:33:20.729915Z",
     "shell.execute_reply.started": "2021-09-27T04:12:02.825838Z"
    },
    "papermill": {
     "duration": 0.029317,
     "end_time": "2021-09-27T05:33:20.730091",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.700774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced20ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T05:33:20.776127Z",
     "iopub.status.busy": "2021-09-27T05:33:20.775460Z",
     "iopub.status.idle": "2021-09-27T06:07:56.915862Z",
     "shell.execute_reply": "2021-09-27T06:07:56.915119Z",
     "shell.execute_reply.started": "2021-09-27T04:12:06.339031Z"
    },
    "papermill": {
     "duration": 2076.164709,
     "end_time": "2021-09-27T06:07:56.916196",
     "exception": false,
     "start_time": "2021-09-27T05:33:20.751487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 112 out of 112 | elapsed: 34.5min finished\n",
      "[Parallel(n_jobs=32)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7e4e551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:56.971714Z",
     "iopub.status.busy": "2021-09-27T06:07:56.970559Z",
     "iopub.status.idle": "2021-09-27T06:07:56.991526Z",
     "shell.execute_reply": "2021-09-27T06:07:56.990924Z",
     "shell.execute_reply.started": "2021-09-27T04:49:42.017589Z"
    },
    "papermill": {
     "duration": 0.05266,
     "end_time": "2021-09-27T06:07:56.991666",
     "exception": false,
     "start_time": "2021-09-27T06:07:56.939006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "\n",
    "train['size_tau_100'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_100'] )\n",
    "test['size_tau_100'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_100'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b1bf852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:57.046575Z",
     "iopub.status.busy": "2021-09-27T06:07:57.045384Z",
     "iopub.status.idle": "2021-09-27T06:07:57.076577Z",
     "shell.execute_reply": "2021-09-27T06:07:57.075986Z",
     "shell.execute_reply.started": "2021-09-27T04:49:42.049003Z"
    },
    "papermill": {
     "duration": 0.062481,
     "end_time": "2021-09-27T06:07:57.076717",
     "exception": false,
     "start_time": "2021-09-27T06:07:57.014236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "train['size_tau2_100'] = np.sqrt( 0.88/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_100'] = np.sqrt( 0.88/ test['trade_order_count_sum'] )\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n",
    "train['size_tau2_d1'] = train['size_tau2_300'] - train['size_tau2_400']\n",
    "test['size_tau2_d1'] = test['size_tau2_300'] - test['size_tau2_400']\n",
    "train['size_tau2_d2'] = train['size_tau2_200'] - train['size_tau2_300']\n",
    "test['size_tau2_d2'] = test['size_tau2_200'] - test['size_tau2_300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d0bf4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:57.130677Z",
     "iopub.status.busy": "2021-09-27T06:07:57.129816Z",
     "iopub.status.idle": "2021-09-27T06:07:57.133521Z",
     "shell.execute_reply": "2021-09-27T06:07:57.133959Z",
     "shell.execute_reply.started": "2021-09-27T04:58:35.033051Z"
    },
    "papermill": {
     "duration": 0.03457,
     "end_time": "2021-09-27T06:07:57.134121",
     "exception": false,
     "start_time": "2021-09-27T06:07:57.099551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30de9c8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:57.190732Z",
     "iopub.status.busy": "2021-09-27T06:07:57.190139Z",
     "iopub.status.idle": "2021-09-27T06:07:59.579806Z",
     "shell.execute_reply": "2021-09-27T06:07:59.579189Z",
     "shell.execute_reply.started": "2021-09-27T04:58:36.678730Z"
    },
    "papermill": {
     "duration": 2.423153,
     "end_time": "2021-09-27T06:07:59.579960",
     "exception": false,
     "start_time": "2021-09-27T06:07:57.156807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089f6bf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:59.639012Z",
     "iopub.status.busy": "2021-09-27T06:07:59.638360Z",
     "iopub.status.idle": "2021-09-27T06:07:59.641263Z",
     "shell.execute_reply": "2021-09-27T06:07:59.640742Z",
     "shell.execute_reply.started": "2021-09-27T04:58:41.407863Z"
    },
    "papermill": {
     "duration": 0.035302,
     "end_time": "2021-09-27T06:07:59.641404",
     "exception": false,
     "start_time": "2021-09-27T06:07:59.606102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_2c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_5c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_2c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_5c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_2c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_5c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_2c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_5c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_2c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_5c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_2c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_5c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_2c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_5c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_2c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_5c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_2c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_5c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_2c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_5c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f742e1e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:59.696678Z",
     "iopub.status.busy": "2021-09-27T06:07:59.695498Z",
     "iopub.status.idle": "2021-09-27T06:07:59.840398Z",
     "shell.execute_reply": "2021-09-27T06:07:59.839556Z",
     "shell.execute_reply.started": "2021-09-27T04:58:42.852309Z"
    },
    "papermill": {
     "duration": 0.175022,
     "end_time": "2021-09-27T06:07:59.840576",
     "exception": false,
     "start_time": "2021-09-27T06:07:59.665554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98484b56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:07:59.898697Z",
     "iopub.status.busy": "2021-09-27T06:07:59.898102Z",
     "iopub.status.idle": "2021-09-27T06:08:05.712656Z",
     "shell.execute_reply": "2021-09-27T06:08:05.712144Z",
     "shell.execute_reply.started": "2021-09-27T04:58:44.872073Z"
    },
    "papermill": {
     "duration": 5.846702,
     "end_time": "2021-09-27T06:08:05.712832",
     "exception": false,
     "start_time": "2021-09-27T06:07:59.866130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "import gc\n",
    "\n",
    "del mat1,mat2\n",
    "#del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "255f031d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:08:05.767037Z",
     "iopub.status.busy": "2021-09-27T06:08:05.766401Z",
     "iopub.status.idle": "2021-09-27T06:08:06.721940Z",
     "shell.execute_reply": "2021-09-27T06:08:06.721337Z",
     "shell.execute_reply.started": "2021-09-27T04:58:51.625638Z"
    },
    "papermill": {
     "duration": 0.984802,
     "end_time": "2021-09-27T06:08:06.722082",
     "exception": false,
     "start_time": "2021-09-27T06:08:05.737280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "anom_list = []\n",
    "for stock_id, tmp_df in train.groupby(['stock_id']):\n",
    "    if np.sum(np.log(tmp_df['target'])<-8.7) > 0:\n",
    "        anom_list.append(stock_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7383e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:08:06.778234Z",
     "iopub.status.busy": "2021-09-27T06:08:06.777389Z",
     "iopub.status.idle": "2021-09-27T06:08:07.831180Z",
     "shell.execute_reply": "2021-09-27T06:08:07.830603Z",
     "shell.execute_reply.started": "2021-09-27T05:06:31.353068Z"
    },
    "papermill": {
     "duration": 1.083388,
     "end_time": "2021-09-27T06:08:07.831326",
     "exception": false,
     "start_time": "2021-09-27T06:08:06.747938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "import copy\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 7,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 3,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':32,\n",
    "    'device': 'cpu',\n",
    "    #'gpu_device_id': 0,\n",
    "    #'gpu_use_dp': True,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.045,        \n",
    "        'lambda_l1': 0.33,\n",
    "        'lambda_l2': 0.85,\n",
    "        'num_leaves': 185,\n",
    "        #'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.97,\n",
    "        #'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.80,\n",
    "        'bagging_freq': 1,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 7,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'device': 'cpu',\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params, eval_idx=True, anom_list=None):\n",
    "    # Hyperparammeters (just basic)\n",
    "    train['weight'] = 1.\n",
    "    if anom_list is not None:\n",
    "        for anom_idx in anom_list:\n",
    "            train.loc[train['stock_id']==anom_idx, 'weight'] = 1e-9\n",
    "\n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"weight\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    #kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    kfold = GroupKFold(n_splits = 5)\n",
    "    y_target = train.target.values\n",
    "    time_id = train.time_id.values\n",
    "    # Iterate through each fold\n",
    "    #for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, y_target, time_id)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = x_train['weight'] / np.square(y_train)\n",
    "        val_weights = x_val['weight'] / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=10000,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=150,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        if eval_idx:\n",
    "            test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'LGBM out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return oof_predictions, test_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536c484e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:08:07.895609Z",
     "iopub.status.busy": "2021-09-27T06:08:07.894997Z",
     "iopub.status.idle": "2021-09-27T06:18:56.724018Z",
     "shell.execute_reply": "2021-09-27T06:18:56.724422Z",
     "shell.execute_reply.started": "2021-09-27T05:06:34.559419Z"
    },
    "papermill": {
     "duration": 648.868238,
     "end_time": "2021-09-27T06:18:56.724604",
     "exception": false,
     "start_time": "2021-09-27T06:08:07.856366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000421737\ttraining's RMSPE: 0.214077\tvalid_1's rmse: 0.000467734\tvalid_1's RMSPE: 0.221815\n",
      "[500]\ttraining's rmse: 0.000399663\ttraining's RMSPE: 0.204192\tvalid_1's rmse: 0.000463096\tvalid_1's RMSPE: 0.219673\n",
      "[750]\ttraining's rmse: 0.000386606\ttraining's RMSPE: 0.198151\tvalid_1's rmse: 0.000461655\tvalid_1's RMSPE: 0.219041\n",
      "[1000]\ttraining's rmse: 0.00037704\ttraining's RMSPE: 0.194012\tvalid_1's rmse: 0.00046073\tvalid_1's RMSPE: 0.218703\n",
      "Early stopping, best iteration is:\n",
      "[1033]\ttraining's rmse: 0.000376075\ttraining's RMSPE: 0.19332\tvalid_1's rmse: 0.000460634\tvalid_1's RMSPE: 0.218502\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000421182\ttraining's RMSPE: 0.207307\tvalid_1's rmse: 0.000468351\tvalid_1's RMSPE: 0.244918\n",
      "[500]\ttraining's rmse: 0.000398882\ttraining's RMSPE: 0.197816\tvalid_1's rmse: 0.000465399\tvalid_1's RMSPE: 0.243393\n",
      "Early stopping, best iteration is:\n",
      "[588]\ttraining's rmse: 0.000393739\ttraining's RMSPE: 0.195611\tvalid_1's rmse: 0.000464908\tvalid_1's RMSPE: 0.243099\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000423602\ttraining's RMSPE: 0.210489\tvalid_1's rmse: 0.00045113\tvalid_1's RMSPE: 0.220835\n",
      "[500]\ttraining's rmse: 0.000400966\ttraining's RMSPE: 0.199651\tvalid_1's rmse: 0.00044872\tvalid_1's RMSPE: 0.219506\n",
      "Early stopping, best iteration is:\n",
      "[563]\ttraining's rmse: 0.000396938\ttraining's RMSPE: 0.197944\tvalid_1's rmse: 0.000448757\tvalid_1's RMSPE: 0.219443\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000422891\ttraining's RMSPE: 0.210266\tvalid_1's rmse: 0.000454993\tvalid_1's RMSPE: 0.230112\n",
      "[500]\ttraining's rmse: 0.000400818\ttraining's RMSPE: 0.200335\tvalid_1's rmse: 0.000453935\tvalid_1's RMSPE: 0.229054\n",
      "Early stopping, best iteration is:\n",
      "[428]\ttraining's rmse: 0.000405961\ttraining's RMSPE: 0.202609\tvalid_1's rmse: 0.000453544\tvalid_1's RMSPE: 0.228733\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000423619\ttraining's RMSPE: 0.212371\tvalid_1's rmse: 0.000452337\tvalid_1's RMSPE: 0.218097\n",
      "[500]\ttraining's rmse: 0.000400937\ttraining's RMSPE: 0.202558\tvalid_1's rmse: 0.000448841\tvalid_1's RMSPE: 0.217416\n",
      "Early stopping, best iteration is:\n",
      "[454]\ttraining's rmse: 0.000404055\ttraining's RMSPE: 0.203604\tvalid_1's rmse: 0.000449077\tvalid_1's RMSPE: 0.217019\n",
      "LGBM out of folds RMSPE is 0.22557098852082402\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "val_predictions_lgb_0 = copy.copy(train[['stock_id', 'time_id', 'target', 'row_id']])\n",
    "val_predictions_lgb_0['predictions'] = 0.\n",
    "test_predictions_lgb_0 = copy.copy(test[['stock_id', 'time_id', 'row_id']])\n",
    "val_lgb_0, test_lgb_0 = train_and_evaluate_lgb(train, test,params0, True, anom_list)\n",
    "val_predictions_lgb_0['predictions'] = val_lgb_0\n",
    "test_predictions_lgb_0['target'] = test_lgb_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e0a8cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:18:56.792740Z",
     "iopub.status.busy": "2021-09-27T06:18:56.791843Z",
     "iopub.status.idle": "2021-09-27T06:32:55.802760Z",
     "shell.execute_reply": "2021-09-27T06:32:55.803257Z"
    },
    "papermill": {
     "duration": 839.046538,
     "end_time": "2021-09-27T06:32:55.803438",
     "exception": false,
     "start_time": "2021-09-27T06:18:56.756900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000406671\ttraining's RMSPE: 0.209156\tvalid_1's rmse: 0.000465817\tvalid_1's RMSPE: 0.221561\n",
      "[500]\ttraining's rmse: 0.000383397\ttraining's RMSPE: 0.201131\tvalid_1's rmse: 0.000463297\tvalid_1's RMSPE: 0.221453\n",
      "Early stopping, best iteration is:\n",
      "[456]\ttraining's rmse: 0.000386231\ttraining's RMSPE: 0.201793\tvalid_1's rmse: 0.000463403\tvalid_1's RMSPE: 0.221182\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000403783\ttraining's RMSPE: 0.200448\tvalid_1's rmse: 0.000466287\tvalid_1's RMSPE: 0.243623\n",
      "[500]\ttraining's rmse: 0.000381887\ttraining's RMSPE: 0.193018\tvalid_1's rmse: 0.000464058\tvalid_1's RMSPE: 0.243631\n",
      "Early stopping, best iteration is:\n",
      "[444]\ttraining's rmse: 0.000385902\ttraining's RMSPE: 0.194141\tvalid_1's rmse: 0.000464284\tvalid_1's RMSPE: 0.24333\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000407296\ttraining's RMSPE: 0.206253\tvalid_1's rmse: 0.00044958\tvalid_1's RMSPE: 0.221373\n",
      "Early stopping, best iteration is:\n",
      "[239]\ttraining's rmse: 0.000408571\ttraining's RMSPE: 0.206749\tvalid_1's rmse: 0.000449717\tvalid_1's RMSPE: 0.221304\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000406475\ttraining's RMSPE: 0.204996\tvalid_1's rmse: 0.00045286\tvalid_1's RMSPE: 0.232044\n",
      "Early stopping, best iteration is:\n",
      "[313]\ttraining's rmse: 0.000398742\ttraining's RMSPE: 0.201759\tvalid_1's rmse: 0.000452281\tvalid_1's RMSPE: 0.231885\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[250]\ttraining's rmse: 0.000407309\ttraining's RMSPE: 0.206493\tvalid_1's rmse: 0.0004503\tvalid_1's RMSPE: 0.219324\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's rmse: 0.000403441\ttraining's RMSPE: 0.204725\tvalid_1's rmse: 0.000449843\tvalid_1's RMSPE: 0.219088\n",
      "LGBM out of folds RMSPE is 0.22754180053600945\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "val_predictions_lgb_1 = copy.copy(train[['stock_id', 'time_id', 'target', 'row_id']])\n",
    "val_predictions_lgb_1['predictions'] = 0.\n",
    "test_predictions_lgb_1 = copy.copy(test[['stock_id', 'time_id', 'row_id']])\n",
    "val_lgb_1, test_lgb_1 = train_and_evaluate_lgb(train, test, params1, True, anom_list)\n",
    "val_predictions_lgb_1['predictions'] = val_lgb_1\n",
    "test_predictions_lgb_1['target'] = test_lgb_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd38f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:32:55.888459Z",
     "iopub.status.busy": "2021-09-27T06:32:55.887779Z",
     "iopub.status.idle": "2021-09-27T06:32:55.897025Z",
     "shell.execute_reply": "2021-09-27T06:32:55.897450Z"
    },
    "papermill": {
     "duration": 0.054594,
     "end_time": "2021-09-27T06:32:55.897619",
     "exception": false,
     "start_time": "2021-09-27T06:32:55.843025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm CV merged: 0.22584874625746357\n"
     ]
    }
   ],
   "source": [
    "merged_predictions = 0.5 * (val_predictions_lgb_0['predictions'] + val_predictions_lgb_1['predictions'])\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)\n",
    "print(\"lgbm CV merged: {}\".format(rmspe(val_predictions_lgb_0['target'], merged_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb3d787c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:32:55.981330Z",
     "iopub.status.busy": "2021-09-27T06:32:55.980667Z",
     "iopub.status.idle": "2021-09-27T06:32:55.992832Z",
     "shell.execute_reply": "2021-09-27T06:32:55.992390Z"
    },
    "papermill": {
     "duration": 0.055848,
     "end_time": "2021-09-27T06:32:55.992984",
     "exception": false,
     "start_time": "2021-09-27T06:32:55.937136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm CV merged and clipped: 0.22584874625746357\n"
     ]
    }
   ],
   "source": [
    "clipped_predictions = np.clip(merged_predictions, 1e-4, 0.072)\n",
    "print(\"lgbm CV merged and clipped: {}\".format(rmspe(val_predictions_lgb_0['target'], clipped_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67c0814b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:32:56.080560Z",
     "iopub.status.busy": "2021-09-27T06:32:56.079854Z",
     "iopub.status.idle": "2021-09-27T06:33:22.587119Z",
     "shell.execute_reply": "2021-09-27T06:33:22.586604Z"
    },
    "papermill": {
     "duration": 26.554576,
     "end_time": "2021-09-27T06:33:22.587286",
     "exception": false,
     "start_time": "2021-09-27T06:32:56.032710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n",
    "\n",
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4924d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:22.679692Z",
     "iopub.status.busy": "2021-09-27T06:33:22.679065Z",
     "iopub.status.idle": "2021-09-27T06:33:24.598760Z",
     "shell.execute_reply": "2021-09-27T06:33:24.598263Z"
    },
    "papermill": {
     "duration": 1.971256,
     "end_time": "2021-09-27T06:33:24.598933",
     "exception": false,
     "start_time": "2021-09-27T06:33:22.627677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c237882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:24.688333Z",
     "iopub.status.busy": "2021-09-27T06:33:24.687577Z",
     "iopub.status.idle": "2021-09-27T06:33:24.821171Z",
     "shell.execute_reply": "2021-09-27T06:33:24.820600Z"
    },
    "papermill": {
     "duration": 0.181085,
     "end_time": "2021-09-27T06:33:24.821320",
     "exception": false,
     "start_time": "2021-09-27T06:33:24.640235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1faabba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:24.911630Z",
     "iopub.status.busy": "2021-09-27T06:33:24.910987Z",
     "iopub.status.idle": "2021-09-27T06:33:28.636246Z",
     "shell.execute_reply": "2021-09-27T06:33:28.635762Z"
    },
    "papermill": {
     "duration": 3.773543,
     "end_time": "2021-09-27T06:33:28.636383",
     "exception": false,
     "start_time": "2021-09-27T06:33:24.862840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13013"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "import gc\n",
    "\n",
    "del mat1,mat2\n",
    "#del train#,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c8d09ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:28.727080Z",
     "iopub.status.busy": "2021-09-27T06:33:28.726265Z",
     "iopub.status.idle": "2021-09-27T06:33:33.617925Z",
     "shell.execute_reply": "2021-09-27T06:33:33.617411Z"
    },
    "papermill": {
     "duration": 4.93944,
     "end_time": "2021-09-27T06:33:33.618076",
     "exception": false,
     "start_time": "2021-09-27T06:33:28.678636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 06:33:29.374140: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2021-09-27 06:33:29.374786: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=50, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=16, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8aab695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:33.717797Z",
     "iopub.status.busy": "2021-09-27T06:33:33.717169Z",
     "iopub.status.idle": "2021-09-27T06:33:33.719896Z",
     "shell.execute_reply": "2021-09-27T06:33:33.720366Z"
    },
    "papermill": {
     "duration": 0.059399,
     "end_time": "2021-09-27T06:33:33.720535",
     "exception": false,
     "start_time": "2021-09-27T06:33:33.661136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "#import tensorflow.keras as keras\n",
    "#from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "#get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "#hidden_units = (128,64,32)\n",
    "#stock_embedding_size = 24\n",
    "nn_params0 = {\n",
    "    'hidden_units': (128,64,32),\n",
    "    'stock_embedding_size': 24,\n",
    "    'input_size': 268,\n",
    "}\n",
    "nn_params1 = {\n",
    "    'hidden_units': (256,128,64),\n",
    "    'stock_embedding_size': 32,\n",
    "    'input_size': 268,\n",
    "}\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model(params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'hidden_units': (128,64,32),\n",
    "            'stock_embedding_size': 24,\n",
    "            'input_size': 268,\n",
    "        }\n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(params['input_size'],), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, params['stock_embedding_size'], \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in params['hidden_units']:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f7af532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:33.811066Z",
     "iopub.status.busy": "2021-09-27T06:33:33.810424Z",
     "iopub.status.idle": "2021-09-27T06:33:33.825261Z",
     "shell.execute_reply": "2021-09-27T06:33:33.825738Z"
    },
    "papermill": {
     "duration": 0.061329,
     "end_time": "2021-09-27T06:33:33.825927",
     "exception": false,
     "start_time": "2021-09-27T06:33:33.764598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_nn(train, test, base_params=None, eval_idx=True):\n",
    "    target_name='target'\n",
    "    scores_folds = {}\n",
    "    model_name = 'NN'\n",
    "    #pred_name = 'pred_{}'.format(model_name)\n",
    "    \n",
    "    n_folds = 5\n",
    "    # kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "    scores_folds[model_name] = []\n",
    "    counter = 1\n",
    "    \n",
    "    features_to_consider = list(train)\n",
    "    \n",
    "    features_to_consider.remove('time_id')\n",
    "    features_to_consider.remove('target')\n",
    "    try:\n",
    "        features_to_consider.remove('pred_NN')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\n",
    "    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n",
    "    \n",
    "    train_predictions_nn = np.zeros(train.shape[0])\n",
    "    test[target_name] = 0\n",
    "    test_predictions_nn = np.zeros(test.shape[0])\n",
    "    kfold = GroupKFold(n_splits = 5)\n",
    "    y_target = train.target.values\n",
    "    time_id = train.time_id.values\n",
    "    # Iterate through each fold\n",
    "    #for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "    for n_count, (trn_ind, val_ind) in enumerate(kfold.split(train, y_target, time_id)):\n",
    "        print(f'Training fold {n_count + 1}')\n",
    "        #print('training: {}'.format(trn_ind))\n",
    "        #print('val: {}'.format(val_ind))\n",
    "        X_train, X_test = train.iloc[trn_ind][features_to_consider], train.iloc[val_ind][features_to_consider]\n",
    "        y_train, y_test = train.iloc[trn_ind][target_name], train.iloc[val_ind][target_name]\n",
    "        #############################################################################################\n",
    "        # NN\n",
    "        #############################################################################################\n",
    "        \n",
    "        model = base_model(base_params)\n",
    "        \n",
    "        model.compile(\n",
    "            keras.optimizers.Adam(learning_rate=0.006),\n",
    "            loss=root_mean_squared_per_error\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            features_to_consider.remove('stock_id')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        num_data = X_train[features_to_consider]\n",
    "        \n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "        num_data = scaler.fit_transform(num_data.values)    \n",
    "        \n",
    "        cat_data = X_train['stock_id']    \n",
    "        target =  y_train\n",
    "        \n",
    "        num_data_test = X_test[features_to_consider]\n",
    "        num_data_test = scaler.transform(num_data_test.values)\n",
    "        cat_data_test = X_test['stock_id']\n",
    "    \n",
    "        model.fit([cat_data, num_data], \n",
    "                  target,               \n",
    "                  batch_size=4096,\n",
    "                  epochs=1000,\n",
    "                  validation_data=([cat_data_test, num_data_test], y_test),\n",
    "                  callbacks=[es, plateau],\n",
    "                  validation_batch_size=len(y_test),\n",
    "                  shuffle=True,\n",
    "                 verbose = 1)\n",
    "    \n",
    "        preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "        train_predictions_nn[val_ind] = preds\n",
    "        score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "        print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "        scores_folds[model_name].append(score)\n",
    "        if eval_idx:\n",
    "            tt =scaler.transform(test[features_to_consider].values)\n",
    "            #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "            test_predictions_nn += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "            #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "           \n",
    "        counter += 1\n",
    "        features_to_consider.append('stock_id')\n",
    "    rmspe_score = rmspe(y_target, train_predictions_nn)\n",
    "    print(\"NN folds RMSPE is: {}\".format(rmspe_score))\n",
    "    return train_predictions_nn, test_predictions_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e3dae13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:33:33.913700Z",
     "iopub.status.busy": "2021-09-27T06:33:33.913144Z",
     "iopub.status.idle": "2021-09-27T06:55:47.376152Z",
     "shell.execute_reply": "2021-09-27T06:55:47.375628Z"
    },
    "papermill": {
     "duration": 1333.508156,
     "end_time": "2021-09-27T06:55:47.376316",
     "exception": false,
     "start_time": "2021-09-27T06:33:33.868160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 06:33:37.094142: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-27 06:33:37.097583: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
      "2021-09-27 06:33:37.097629: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-27 06:33:37.097673: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (502fef7f5cfb): /proc/driver/nvidia/version does not exist\n",
      "2021-09-27 06:33:37.098157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-27 06:33:37.098519: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-27 06:33:40.778286: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-27 06:33:40.791992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200140000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 33ms/step - loss: 40.5928 - val_loss: 3.6650\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 2.3450 - val_loss: 0.5840\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.9843 - val_loss: 0.6356\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.7321 - val_loss: 0.7185\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6417 - val_loss: 0.6811\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6218 - val_loss: 0.4742\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.5950 - val_loss: 0.6155\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6133 - val_loss: 0.4648\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6022 - val_loss: 0.5336\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5898 - val_loss: 0.8180\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.6428 - val_loss: 0.6255\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5493 - val_loss: 0.6166\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5729 - val_loss: 0.5059\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6305 - val_loss: 0.6407\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5888 - val_loss: 0.4166\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5350 - val_loss: 0.5475\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.4913 - val_loss: 0.5097\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4780 - val_loss: 0.4335\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4562 - val_loss: 0.4878\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6542 - val_loss: 1.1709\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.7913 - val_loss: 0.3850\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3767 - val_loss: 0.4435\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3769 - val_loss: 0.3485\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3662 - val_loss: 0.3641\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3622 - val_loss: 0.3673\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3537 - val_loss: 0.3291\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3585 - val_loss: 0.3542\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3545 - val_loss: 0.3422\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3596 - val_loss: 0.4300\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.6874 - val_loss: 0.3945\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3847 - val_loss: 0.2964\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.3010 - val_loss: 0.2302\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2797 - val_loss: 0.2177\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2157 - val_loss: 0.2274\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2134 - val_loss: 0.2182\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2119 - val_loss: 0.2243\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2137 - val_loss: 0.2163\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2119 - val_loss: 0.2196\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2158 - val_loss: 0.2306\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2149 - val_loss: 0.2216\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2345 - val_loss: 0.2262\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2234 - val_loss: 0.2286\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2349 - val_loss: 0.2198\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2238 - val_loss: 0.2256\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2228 - val_loss: 0.2397\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2294 - val_loss: 0.2371\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2555 - val_loss: 0.2827\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2334 - val_loss: 0.2507\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2261 - val_loss: 0.2631\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2372 - val_loss: 0.2329\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2303 - val_loss: 0.2755\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2273 - val_loss: 0.2183\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2270 - val_loss: 0.2960\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2189 - val_loss: 0.2145\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2021 - val_loss: 0.2146\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2025 - val_loss: 0.2147\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2022 - val_loss: 0.2138\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2010 - val_loss: 0.2141\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2007 - val_loss: 0.2149\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2017 - val_loss: 0.2153\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2009 - val_loss: 0.2150\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2014 - val_loss: 0.2141\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2009 - val_loss: 0.2145\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2005 - val_loss: 0.2148\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2006 - val_loss: 0.2146\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2004 - val_loss: 0.2140\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2000 - val_loss: 0.2136\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2000 - val_loss: 0.2145\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2010 - val_loss: 0.2159\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2002 - val_loss: 0.2147\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2001 - val_loss: 0.2153\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2011 - val_loss: 0.2182\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2003 - val_loss: 0.2163\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1994 - val_loss: 0.2160\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.1999 - val_loss: 0.2161\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2004 - val_loss: 0.2173\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2002 - val_loss: 0.2165\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2002 - val_loss: 0.2167\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1999 - val_loss: 0.2166\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2001 - val_loss: 0.2154\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2006 - val_loss: 0.2144\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2002 - val_loss: 0.2238\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2062 - val_loss: 0.2147\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1970 - val_loss: 0.2146\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1977 - val_loss: 0.2140\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1964 - val_loss: 0.2145\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1968 - val_loss: 0.2148\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1968 - val_loss: 0.2148\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1973 - val_loss: 0.2138\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1959 - val_loss: 0.2151\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1969 - val_loss: 0.2147\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1965 - val_loss: 0.2151\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1973 - val_loss: 0.2149\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1967 - val_loss: 0.2142\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1963 - val_loss: 0.2144\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1966 - val_loss: 0.2147\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1973 - val_loss: 0.2145\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2141\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2150\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1960 - val_loss: 0.2144\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1957 - val_loss: 0.2148\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1959 - val_loss: 0.2147\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1952 - val_loss: 0.2147\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1956 - val_loss: 0.2149\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1961 - val_loss: 0.2146\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1951 - val_loss: 0.2145\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1960 - val_loss: 0.2147\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1957 - val_loss: 0.2147\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1956 - val_loss: 0.2146\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1957 - val_loss: 0.2149\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1956 - val_loss: 0.2148\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1959 - val_loss: 0.2146\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1956 - val_loss: 0.2149\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2151\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1961 - val_loss: 0.2147\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1954 - val_loss: 0.2146\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1953 - val_loss: 0.2147\n",
      "Fold 1 NN: 0.21362\n",
      "Training fold 2\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 3s 29ms/step - loss: 41.6484 - val_loss: 2.2581\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 1.3863 - val_loss: 0.8950\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.7078 - val_loss: 1.1182\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.8610 - val_loss: 0.8124\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.7614 - val_loss: 0.8893\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.8333 - val_loss: 0.6330\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.9590 - val_loss: 0.9740\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.7129 - val_loss: 0.6650\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.6360 - val_loss: 0.8841\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6646 - val_loss: 1.3225\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 1.8330 - val_loss: 0.7850\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5944 - val_loss: 0.6059\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.5552 - val_loss: 0.7960\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.6005 - val_loss: 0.6650\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5381 - val_loss: 0.7513\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5469 - val_loss: 0.6070\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 1.0852 - val_loss: 0.7356\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5901 - val_loss: 0.5476\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4464 - val_loss: 0.4787\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4403 - val_loss: 0.4856\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.4376 - val_loss: 0.4902\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.4266 - val_loss: 0.3618\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4079 - val_loss: 0.5155\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4350 - val_loss: 0.4749\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4011 - val_loss: 0.4490\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6132 - val_loss: 0.9750\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6856 - val_loss: 0.3899\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3040 - val_loss: 0.2379\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2186 - val_loss: 0.2362\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2206 - val_loss: 0.2529\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2144 - val_loss: 0.2285\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2176 - val_loss: 0.2272\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2144 - val_loss: 0.2311\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2208 - val_loss: 0.2420\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2374 - val_loss: 0.2423\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2192 - val_loss: 0.2312\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 35ms/step - loss: 0.2348 - val_loss: 0.2357\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 0.2301 - val_loss: 0.2944\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 3s 33ms/step - loss: 0.2292 - val_loss: 0.2492\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2418 - val_loss: 0.2328\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2314 - val_loss: 0.2746\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2362 - val_loss: 0.2358\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2328 - val_loss: 0.2521\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2424 - val_loss: 0.2563\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2458 - val_loss: 0.3349\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2430 - val_loss: 0.3370\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2492 - val_loss: 0.2368\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2451 - val_loss: 0.2325\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2036 - val_loss: 0.2237\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2019 - val_loss: 0.2233\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2011 - val_loss: 0.2230\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2016 - val_loss: 0.2252\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2017 - val_loss: 0.2234\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2016 - val_loss: 0.2241\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2012 - val_loss: 0.2248\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1997 - val_loss: 0.2243\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2009 - val_loss: 0.2238\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2003 - val_loss: 0.2243\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1998 - val_loss: 0.2229\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1998 - val_loss: 0.2257\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2003 - val_loss: 0.2251\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2006 - val_loss: 0.2246\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1992 - val_loss: 0.2251\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2002 - val_loss: 0.2273\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2006 - val_loss: 0.2255\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2004 - val_loss: 0.2247\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1997 - val_loss: 0.2234\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1989 - val_loss: 0.2244\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1979 - val_loss: 0.2241\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1985 - val_loss: 0.2237\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1970 - val_loss: 0.2228\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1978 - val_loss: 0.2246\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1981 - val_loss: 0.2238\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1975 - val_loss: 0.2235\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1976 - val_loss: 0.2239\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1975 - val_loss: 0.2240\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1979 - val_loss: 0.2247\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1979 - val_loss: 0.2241\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1981 - val_loss: 0.2235\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1975 - val_loss: 0.2237\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1975 - val_loss: 0.2243\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1978 - val_loss: 0.2242\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1977 - val_loss: 0.2233\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1971 - val_loss: 0.2244\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1981 - val_loss: 0.2249\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1973 - val_loss: 0.2237\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1982 - val_loss: 0.2257\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1972 - val_loss: 0.2238\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1966 - val_loss: 0.2235\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1967 - val_loss: 0.2235\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1968 - val_loss: 0.2244\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1963 - val_loss: 0.2233\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2238\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1966 - val_loss: 0.2244\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2238\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1965 - val_loss: 0.2247\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1960 - val_loss: 0.2244\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2239\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1969 - val_loss: 0.2241\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2245\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1968 - val_loss: 0.2238\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1963 - val_loss: 0.2238\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1961 - val_loss: 0.2234\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1963 - val_loss: 0.2242\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1963 - val_loss: 0.2237\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2241\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2239\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1957 - val_loss: 0.2240\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1963 - val_loss: 0.2240\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2243\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1958 - val_loss: 0.2237\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1963 - val_loss: 0.2238\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1967 - val_loss: 0.2240\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2240\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1961 - val_loss: 0.2241\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1964 - val_loss: 0.2242\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1961 - val_loss: 0.2241\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1962 - val_loss: 0.2242\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1960 - val_loss: 0.2240\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1955 - val_loss: 0.2239\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1963 - val_loss: 0.2239\n",
      "Fold 2 NN: 0.22277\n",
      "Training fold 3\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 3s 27ms/step - loss: 35.9870 - val_loss: 2.4287\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 1.0511 - val_loss: 0.7323\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.7384 - val_loss: 0.8127\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6503 - val_loss: 0.7987\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.6486 - val_loss: 0.7091\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.6049 - val_loss: 0.5270\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5700 - val_loss: 1.2178\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.9034 - val_loss: 0.5894\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5237 - val_loss: 0.6787\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5296 - val_loss: 0.5436\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4903 - val_loss: 0.4676\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5091 - val_loss: 0.5074\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.4781 - val_loss: 2.1656\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.9241 - val_loss: 0.4753\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4397 - val_loss: 0.4387\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4063 - val_loss: 0.4236\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4084 - val_loss: 0.4227\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4224 - val_loss: 0.4127\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4090 - val_loss: 0.4023\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3793 - val_loss: 0.3614\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.3659 - val_loss: 0.4043\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.6544 - val_loss: 1.3990\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5991 - val_loss: 0.2325\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2240 - val_loss: 0.2238\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2253 - val_loss: 0.2158\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2172 - val_loss: 0.2178\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2164 - val_loss: 0.2239\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2167 - val_loss: 0.2162\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2206 - val_loss: 0.2129\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2164 - val_loss: 0.2200\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2264 - val_loss: 0.2594\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2355 - val_loss: 0.2261\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2246 - val_loss: 0.2189\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2160 - val_loss: 0.3152\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2711 - val_loss: 0.2356\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2305 - val_loss: 0.2335\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2314 - val_loss: 0.2417\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2278 - val_loss: 0.2239\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2393 - val_loss: 0.2165\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2300 - val_loss: 0.2487\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2336 - val_loss: 0.2179\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2287 - val_loss: 0.2823\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2350 - val_loss: 0.2161\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2346 - val_loss: 0.2690\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2318 - val_loss: 0.2119\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2501 - val_loss: 0.2212\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2377 - val_loss: 0.2427\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2322 - val_loss: 0.2970\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2492 - val_loss: 0.2414\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2349 - val_loss: 0.2175\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2308 - val_loss: 0.2670\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2567 - val_loss: 0.2620\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 1.8307 - val_loss: 0.3060\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2515 - val_loss: 0.2244\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2252 - val_loss: 0.2196\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2262 - val_loss: 0.2231\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2216 - val_loss: 0.2556\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2230 - val_loss: 0.2138\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2223 - val_loss: 0.2135\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2218 - val_loss: 0.2140\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2237 - val_loss: 0.2976\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2262 - val_loss: 0.2104\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2073 - val_loss: 0.2098\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2065 - val_loss: 0.2125\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2078 - val_loss: 0.2103\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2074 - val_loss: 0.2125\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2059 - val_loss: 0.2111\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2069 - val_loss: 0.2103\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2067 - val_loss: 0.2098\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2065 - val_loss: 0.2107\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2080 - val_loss: 0.2098\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2061 - val_loss: 0.2108\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2063 - val_loss: 0.2103\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2071 - val_loss: 0.2104\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2072 - val_loss: 0.2120\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2062 - val_loss: 0.2107\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2062 - val_loss: 0.2115\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2054 - val_loss: 0.2106\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2049 - val_loss: 0.2101\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2041 - val_loss: 0.2090\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2040 - val_loss: 0.2092\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2040 - val_loss: 0.2095\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2042 - val_loss: 0.2097\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2046 - val_loss: 0.2098\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2044 - val_loss: 0.2096\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2040 - val_loss: 0.2092\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2039 - val_loss: 0.2100\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2039 - val_loss: 0.2095\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2036 - val_loss: 0.2099\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2036 - val_loss: 0.2096\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2035 - val_loss: 0.2092\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2036 - val_loss: 0.2093\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2032 - val_loss: 0.2092\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2032 - val_loss: 0.2106\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2034 - val_loss: 0.2094\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2030 - val_loss: 0.2095\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2025 - val_loss: 0.2096\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2029 - val_loss: 0.2098\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2031 - val_loss: 0.2093\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2033 - val_loss: 0.2095\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2027 - val_loss: 0.2095\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2032 - val_loss: 0.2092\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2037 - val_loss: 0.2093\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2028 - val_loss: 0.2093\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2031 - val_loss: 0.2094\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2032 - val_loss: 0.2098\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2028 - val_loss: 0.2095\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2031 - val_loss: 0.2094\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2030 - val_loss: 0.2096\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2031 - val_loss: 0.2092\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2033 - val_loss: 0.2094\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2033 - val_loss: 0.2095\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2032 - val_loss: 0.2094\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2038 - val_loss: 0.2095\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2032 - val_loss: 0.2095\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2034 - val_loss: 0.2094\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2029 - val_loss: 0.2096\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2026 - val_loss: 0.2096\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2020 - val_loss: 0.2095\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2030 - val_loss: 0.2095\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2026 - val_loss: 0.2094\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2021 - val_loss: 0.2097\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2026 - val_loss: 0.2095\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2026 - val_loss: 0.2095\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2024 - val_loss: 0.2094\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2032 - val_loss: 0.2095\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2029 - val_loss: 0.2095\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2022 - val_loss: 0.2096\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2024 - val_loss: 0.2094\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2034 - val_loss: 0.2094\n",
      "Fold 3 NN: 0.20904\n",
      "Training fold 4\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 3s 28ms/step - loss: 42.9804 - val_loss: 2.4121\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 1.9842 - val_loss: 1.3336\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 1.1554 - val_loss: 1.0123\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.8895 - val_loss: 0.8433\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.7802 - val_loss: 0.7973\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.6830 - val_loss: 0.5302\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6285 - val_loss: 0.5126\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5999 - val_loss: 0.6284\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5852 - val_loss: 0.6071\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5927 - val_loss: 0.7094\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.6158 - val_loss: 0.5283\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5359 - val_loss: 0.6671\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.5764 - val_loss: 0.6670\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5524 - val_loss: 0.5428\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5203 - val_loss: 0.5467\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4950 - val_loss: 0.4768\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4831 - val_loss: 1.6823\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 1.8688 - val_loss: 0.2910\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2736 - val_loss: 0.2289\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2306 - val_loss: 0.2411\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2296 - val_loss: 0.2252\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2373 - val_loss: 0.2226\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2309 - val_loss: 0.2260\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2386 - val_loss: 0.2402\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2404 - val_loss: 0.2286\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2387 - val_loss: 0.2294\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2498 - val_loss: 0.2176\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2475 - val_loss: 0.2309\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2674 - val_loss: 0.2268\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2511 - val_loss: 0.3645\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2636 - val_loss: 0.2638\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2523 - val_loss: 0.2604\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2611 - val_loss: 0.2309\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2458 - val_loss: 0.2792\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2606 - val_loss: 0.2944\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2581 - val_loss: 0.2587\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2524 - val_loss: 0.2720\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2518 - val_loss: 0.2389\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2593 - val_loss: 0.2476\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2486 - val_loss: 0.2167\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2415 - val_loss: 0.2211\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2541 - val_loss: 0.2592\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2543 - val_loss: 0.2817\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2518 - val_loss: 0.2260\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2435 - val_loss: 0.2338\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2425 - val_loss: 0.3176\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2735 - val_loss: 0.2584\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2361 - val_loss: 0.2279\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2437 - val_loss: 0.2218\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2410 - val_loss: 0.2637\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2846 - val_loss: 0.2991\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2301 - val_loss: 0.2225\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2238 - val_loss: 0.3340\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2560 - val_loss: 0.2307\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2239 - val_loss: 0.2196\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2279 - val_loss: 0.2122\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2315 - val_loss: 0.2763\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4020 - val_loss: 0.6961\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 2.5711 - val_loss: 0.2651\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2462 - val_loss: 0.2207\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2271 - val_loss: 0.2184\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2246 - val_loss: 0.2200\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2243 - val_loss: 0.2140\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2222 - val_loss: 0.2157\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2257 - val_loss: 0.2136\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2325 - val_loss: 0.2192\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2177 - val_loss: 0.2476\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2227 - val_loss: 0.2118\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2241 - val_loss: 0.3104\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2340 - val_loss: 0.2352\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2318 - val_loss: 0.2358\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2221 - val_loss: 0.2207\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2250 - val_loss: 0.2146\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2285 - val_loss: 0.2240\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2211 - val_loss: 0.2115\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2243 - val_loss: 0.2473\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2360 - val_loss: 0.2308\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2325 - val_loss: 0.2170\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2283 - val_loss: 0.2108\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2257 - val_loss: 0.2231\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2321 - val_loss: 0.2334\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2214 - val_loss: 0.2098\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 3s 34ms/step - loss: 0.2212 - val_loss: 0.2194\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 3s 33ms/step - loss: 0.2261 - val_loss: 0.2379\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.2209 - val_loss: 0.2861\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2302 - val_loss: 0.2220\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2268 - val_loss: 0.2145\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2219 - val_loss: 0.2408\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2239 - val_loss: 0.2139\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2267 - val_loss: 0.2613\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2248 - val_loss: 0.2097\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2231 - val_loss: 0.2271\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2270 - val_loss: 0.2224\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2570 - val_loss: 0.3980\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2535 - val_loss: 0.2310\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2211 - val_loss: 0.2211\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2247 - val_loss: 0.2514\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2499 - val_loss: 0.4138\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2441 - val_loss: 0.2087\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2030 - val_loss: 0.2081\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2017 - val_loss: 0.2082\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2016 - val_loss: 0.2084\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2023 - val_loss: 0.2083\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2015 - val_loss: 0.2076\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2011 - val_loss: 0.2080\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2006 - val_loss: 0.2089\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2009 - val_loss: 0.2093\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2011 - val_loss: 0.2102\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2015 - val_loss: 0.2085\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2002 - val_loss: 0.2079\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2006 - val_loss: 0.2100\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2009 - val_loss: 0.2091\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2010 - val_loss: 0.2096\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2010 - val_loss: 0.2089\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2000 - val_loss: 0.2085\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2002 - val_loss: 0.2081\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1996 - val_loss: 0.2090\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2002 - val_loss: 0.2087\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1992 - val_loss: 0.2088\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2005 - val_loss: 0.2113\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1996 - val_loss: 0.2080\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1977 - val_loss: 0.2083\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1983 - val_loss: 0.2088\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1981 - val_loss: 0.2081\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1980 - val_loss: 0.2085\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1987 - val_loss: 0.2086\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1979 - val_loss: 0.2086\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1981 - val_loss: 0.2084\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1978 - val_loss: 0.2082\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1983 - val_loss: 0.2084\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1982 - val_loss: 0.2085\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1984 - val_loss: 0.2085\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1981 - val_loss: 0.2084\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1983 - val_loss: 0.2085\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1984 - val_loss: 0.2085\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1986 - val_loss: 0.2105\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1980 - val_loss: 0.2084\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1975 - val_loss: 0.2085\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1972 - val_loss: 0.2084\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1969 - val_loss: 0.2085\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1978 - val_loss: 0.2085\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 2s 29ms/step - loss: 0.1973 - val_loss: 0.2088\n",
      "Epoch 143/1000\n",
      "84/84 [==============================] - 3s 31ms/step - loss: 0.1974 - val_loss: 0.2086\n",
      "Epoch 144/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1972 - val_loss: 0.2086\n",
      "Epoch 145/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1976 - val_loss: 0.2086\n",
      "Epoch 146/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1978 - val_loss: 0.2089\n",
      "Epoch 147/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1972 - val_loss: 0.2085\n",
      "Epoch 148/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1980 - val_loss: 0.2086\n",
      "Epoch 149/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1975 - val_loss: 0.2086\n",
      "Epoch 150/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1970 - val_loss: 0.2089\n",
      "Epoch 151/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1972 - val_loss: 0.2084\n",
      "Epoch 152/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1979 - val_loss: 0.2086\n",
      "Epoch 153/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1969 - val_loss: 0.2088\n",
      "Epoch 154/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1970 - val_loss: 0.2087\n",
      "Fold 4 NN: 0.20763\n",
      "Training fold 5\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 38ms/step - loss: 53.1975 - val_loss: 2.3540\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 1.3266 - val_loss: 0.4243\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.7172 - val_loss: 0.8272\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.7353 - val_loss: 0.7252\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6307 - val_loss: 0.6601\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5968 - val_loss: 0.6666\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5805 - val_loss: 0.6104\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5711 - val_loss: 0.5226\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6831 - val_loss: 0.5884\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5403 - val_loss: 0.5637\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.5154 - val_loss: 0.5608\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5215 - val_loss: 0.6128\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5254 - val_loss: 0.8302\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 1.2427 - val_loss: 0.4942\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4542 - val_loss: 0.3992\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4369 - val_loss: 0.4949\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.4445 - val_loss: 0.4557\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.4298 - val_loss: 0.4730\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.4172 - val_loss: 0.4941\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.4191 - val_loss: 0.4107\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.4046 - val_loss: 0.4133\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.4037 - val_loss: 0.4517\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4091 - val_loss: 0.5678\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.5356 - val_loss: 0.4899\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.3941 - val_loss: 0.3816\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.3707 - val_loss: 0.3723\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.3592 - val_loss: 0.3368\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3659 - val_loss: 0.3948\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3526 - val_loss: 0.4676\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.4490 - val_loss: 0.4222\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.3579 - val_loss: 0.3026\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3287 - val_loss: 0.3221\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.3311 - val_loss: 0.3315\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3354 - val_loss: 0.3491\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.3210 - val_loss: 0.3192\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.3240 - val_loss: 0.3215\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.3292 - val_loss: 1.0970\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 1.5475 - val_loss: 0.2287\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2242 - val_loss: 0.2209\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2185 - val_loss: 0.2152\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2136 - val_loss: 0.2140\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2184 - val_loss: 0.2238\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2216 - val_loss: 0.2212\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2138 - val_loss: 0.2135\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2113 - val_loss: 0.2208\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2140 - val_loss: 0.2111\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2146 - val_loss: 0.2116\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2197 - val_loss: 0.2178\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2158 - val_loss: 0.2130\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2206 - val_loss: 0.2141\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2196 - val_loss: 0.2146\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2372 - val_loss: 0.2107\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2184 - val_loss: 0.2317\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.2306 - val_loss: 0.2353\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2261 - val_loss: 0.2306\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2183 - val_loss: 0.2369\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2189 - val_loss: 0.2392\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2221 - val_loss: 0.2271\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2240 - val_loss: 0.2527\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2323 - val_loss: 0.2193\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.2447 - val_loss: 0.2181\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2207 - val_loss: 0.2312\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2389 - val_loss: 0.2226\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.2267 - val_loss: 0.2110\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2247 - val_loss: 0.2183\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2339 - val_loss: 0.2162\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2420 - val_loss: 0.2197\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2240 - val_loss: 0.2135\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2042 - val_loss: 0.2089\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2039 - val_loss: 0.2090\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2023 - val_loss: 0.2092\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2023 - val_loss: 0.2085\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.2017 - val_loss: 0.2096\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.2018 - val_loss: 0.2098\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2020 - val_loss: 0.2095\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2009 - val_loss: 0.2093\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2009 - val_loss: 0.2097\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2007 - val_loss: 0.2116\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2019 - val_loss: 0.2092\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2015 - val_loss: 0.2116\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2016 - val_loss: 0.2106\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2008 - val_loss: 0.2112\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2006 - val_loss: 0.2103\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2005 - val_loss: 0.2120\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2018 - val_loss: 0.2116\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2008 - val_loss: 0.2119\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.2008 - val_loss: 0.2156\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.2012 - val_loss: 0.2092\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1994 - val_loss: 0.2098\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1986 - val_loss: 0.2115\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1983 - val_loss: 0.2108\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 2s 27ms/step - loss: 0.1985 - val_loss: 0.2101\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1989 - val_loss: 0.2106\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1986 - val_loss: 0.2103\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1992 - val_loss: 0.2110\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1986 - val_loss: 0.2116\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1985 - val_loss: 0.2117\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1988 - val_loss: 0.2105\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1983 - val_loss: 0.2104\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1985 - val_loss: 0.2112\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1988 - val_loss: 0.2127\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1988 - val_loss: 0.2115\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1987 - val_loss: 0.2117\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1985 - val_loss: 0.2128\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1981 - val_loss: 0.2111\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1976 - val_loss: 0.2107\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1971 - val_loss: 0.2112\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1982 - val_loss: 0.2114\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.1978 - val_loss: 0.2111\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 2s 28ms/step - loss: 0.1979 - val_loss: 0.2111\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 2s 26ms/step - loss: 0.1977 - val_loss: 0.2112\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1979 - val_loss: 0.2116\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1983 - val_loss: 0.2114\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 2s 24ms/step - loss: 0.1978 - val_loss: 0.2112\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 2s 25ms/step - loss: 0.1974 - val_loss: 0.2111\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1982 - val_loss: 0.2117\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.1973 - val_loss: 0.2114\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1974 - val_loss: 0.2115\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1974 - val_loss: 0.2114\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1983 - val_loss: 0.2110\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1978 - val_loss: 0.2113\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.1972 - val_loss: 0.2113\n",
      "Fold 5 NN: 0.20854\n",
      "NN folds RMSPE is: 0.21239381057830192\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "val_predictions_nn_0 = copy.copy(train_nn[['stock_id', 'time_id', 'target']])\n",
    "val_predictions_nn_0['predictions'] = 0.\n",
    "test_predictions_nn_0 = copy.copy(test_nn[['stock_id', 'time_id']])\n",
    "val_nn_0, test_nn_0 = train_and_evaluate_nn(train_nn, test_nn, nn_params0, True)\n",
    "val_predictions_nn_0['predictions'] = val_nn_0\n",
    "test_predictions_nn_0['target'] = test_nn_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ed89f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T06:55:56.253488Z",
     "iopub.status.busy": "2021-09-27T06:55:56.252849Z",
     "iopub.status.idle": "2021-09-27T07:35:35.660682Z",
     "shell.execute_reply": "2021-09-27T07:35:35.661210Z"
    },
    "papermill": {
     "duration": 2383.863264,
     "end_time": "2021-09-27T07:35:35.661412",
     "exception": false,
     "start_time": "2021-09-27T06:55:51.798148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 86.3977 - val_loss: 10.4478\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 4.5700 - val_loss: 2.6491\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 5.3918 - val_loss: 8.1808\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 3.1986 - val_loss: 1.1894\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 1.4986 - val_loss: 3.6919\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 4.9792 - val_loss: 8.3392\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 5.1459 - val_loss: 6.4743\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 3.7412 - val_loss: 1.2576\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 3.3331 - val_loss: 1.6966\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 1.8977 - val_loss: 2.4023\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.8658 - val_loss: 1.9969\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.8588 - val_loss: 2.0359\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.8534 - val_loss: 2.1320\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 2.8529 - val_loss: 2.6987\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.8851 - val_loss: 1.5567\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 1.8088 - val_loss: 1.9005\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 1.8272 - val_loss: 1.7006\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 3.1754 - val_loss: 1.2321\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 1.7996 - val_loss: 1.6004\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 1.7981 - val_loss: 1.6821\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.6330 - val_loss: 0.2326\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.2267 - val_loss: 0.2303\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2499 - val_loss: 0.2310\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2302 - val_loss: 0.2462\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2322 - val_loss: 0.2254\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2271 - val_loss: 0.2374\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.2398 - val_loss: 0.2474\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.2450 - val_loss: 0.2326\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2467 - val_loss: 0.2382\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2535 - val_loss: 0.2445\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2526 - val_loss: 0.2325\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2552 - val_loss: 0.2441\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2659 - val_loss: 0.3110\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2758 - val_loss: 0.2649\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2753 - val_loss: 0.2681\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2715 - val_loss: 0.2583\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2753 - val_loss: 0.2745\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2749 - val_loss: 0.3635\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2763 - val_loss: 0.3173\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2765 - val_loss: 0.3236\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2814 - val_loss: 0.3299\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2320 - val_loss: 0.2187\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2107 - val_loss: 0.2186\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2099 - val_loss: 0.2206\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2099 - val_loss: 0.2185\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 0.2099 - val_loss: 0.2184\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2098 - val_loss: 0.2185\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.2110 - val_loss: 0.2182\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2090 - val_loss: 0.2182\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2090 - val_loss: 0.2191\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2094 - val_loss: 0.2185\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2096 - val_loss: 0.2182\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2092 - val_loss: 0.2180\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.2092 - val_loss: 0.2229\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 6s 75ms/step - loss: 0.2095 - val_loss: 0.2175\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2089 - val_loss: 0.2174\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2092 - val_loss: 0.2206\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2083 - val_loss: 0.2196\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2080 - val_loss: 0.2196\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2091 - val_loss: 0.2177\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2091 - val_loss: 0.2182\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2081 - val_loss: 0.2199\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2082 - val_loss: 0.2167\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2079 - val_loss: 0.2176\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2093 - val_loss: 0.2166\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2093 - val_loss: 0.2288\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2109 - val_loss: 0.2172\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2101 - val_loss: 0.2272\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2118 - val_loss: 0.2171\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2078 - val_loss: 0.2167\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2087 - val_loss: 0.2224\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2102 - val_loss: 0.2213\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2103 - val_loss: 0.2162\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2065 - val_loss: 0.2182\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2074 - val_loss: 0.2199\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2085 - val_loss: 0.2176\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2112 - val_loss: 0.2212\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2106 - val_loss: 0.2298\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2101 - val_loss: 0.2168\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2076 - val_loss: 0.2177\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2080 - val_loss: 0.2391\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2097 - val_loss: 0.2216\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2082 - val_loss: 0.2201\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2068 - val_loss: 0.2159\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2106 - val_loss: 0.2184\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2061 - val_loss: 0.2180\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2151 - val_loss: 0.2160\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2072 - val_loss: 0.2170\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2076 - val_loss: 0.2180\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2126 - val_loss: 0.2171\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2102 - val_loss: 0.2167\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2110 - val_loss: 0.2161\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2067 - val_loss: 0.2169\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2052 - val_loss: 0.2152\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2079 - val_loss: 0.2172\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2066 - val_loss: 0.2180\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2041 - val_loss: 0.2195\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2063 - val_loss: 0.2275\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2058 - val_loss: 0.2161\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2081 - val_loss: 0.2177\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2054 - val_loss: 0.2175\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2023 - val_loss: 0.2163\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.2027 - val_loss: 0.2241\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2053 - val_loss: 0.2169\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 0.2042 - val_loss: 0.2341\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2081 - val_loss: 0.2216\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2181 - val_loss: 0.2189\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2106 - val_loss: 0.2174\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2030 - val_loss: 0.2181\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2076 - val_loss: 0.2338\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2042 - val_loss: 0.2147\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1993 - val_loss: 0.2147\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1996 - val_loss: 0.2145\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1996 - val_loss: 0.2144\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1992 - val_loss: 0.2147\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1989 - val_loss: 0.2149\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1989 - val_loss: 0.2148\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1985 - val_loss: 0.2147\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.1993 - val_loss: 0.2143\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1984 - val_loss: 0.2156\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1987 - val_loss: 0.2148\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1989 - val_loss: 0.2154\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 0.1993 - val_loss: 0.2143\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.1988 - val_loss: 0.2145\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.1988 - val_loss: 0.2149\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1986 - val_loss: 0.2152\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.1985 - val_loss: 0.2151\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1979 - val_loss: 0.2148\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1983 - val_loss: 0.2152\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1986 - val_loss: 0.2152\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1988 - val_loss: 0.2157\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1980 - val_loss: 0.2149\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1981 - val_loss: 0.2160\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1978 - val_loss: 0.2159\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1976 - val_loss: 0.2152\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1979 - val_loss: 0.2154\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1974 - val_loss: 0.2152\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1983 - val_loss: 0.2154\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1973 - val_loss: 0.2150\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1966 - val_loss: 0.2148\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1964 - val_loss: 0.2150\n",
      "Epoch 143/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1971 - val_loss: 0.2148\n",
      "Epoch 144/1000\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 0.1967 - val_loss: 0.2151\n",
      "Epoch 145/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1975 - val_loss: 0.2150\n",
      "Epoch 146/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.1963 - val_loss: 0.2150\n",
      "Epoch 147/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1967 - val_loss: 0.2149\n",
      "Epoch 148/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1970 - val_loss: 0.2149\n",
      "Epoch 149/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1969 - val_loss: 0.2149\n",
      "Epoch 150/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1968 - val_loss: 0.2151\n",
      "Epoch 151/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1967 - val_loss: 0.2150\n",
      "Epoch 152/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1972 - val_loss: 0.2150\n",
      "Epoch 153/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1966 - val_loss: 0.2150\n",
      "Epoch 154/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1970 - val_loss: 0.2149\n",
      "Epoch 155/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1973 - val_loss: 0.2150\n",
      "Epoch 156/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1970 - val_loss: 0.2151\n",
      "Epoch 157/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1965 - val_loss: 0.2150\n",
      "Epoch 158/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2150\n",
      "Epoch 159/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1968 - val_loss: 0.2149\n",
      "Epoch 160/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1972 - val_loss: 0.2150\n",
      "Epoch 161/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1962 - val_loss: 0.2150\n",
      "Epoch 162/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1963 - val_loss: 0.2150\n",
      "Epoch 163/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1967 - val_loss: 0.2151\n",
      "Epoch 164/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.1969 - val_loss: 0.2150\n",
      "Epoch 165/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1967 - val_loss: 0.2149\n",
      "Epoch 166/1000\n",
      "84/84 [==============================] - 4s 53ms/step - loss: 0.1964 - val_loss: 0.2149\n",
      "Epoch 167/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2150\n",
      "Epoch 168/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1960 - val_loss: 0.2150\n",
      "Epoch 169/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1963 - val_loss: 0.2150\n",
      "Epoch 170/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2150\n",
      "Epoch 171/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1970 - val_loss: 0.2150\n",
      "Epoch 172/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1968 - val_loss: 0.2150\n",
      "Epoch 173/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2150\n",
      "Epoch 174/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1963 - val_loss: 0.2150\n",
      "Fold 1 NN: 0.21433\n",
      "Training fold 2\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 5s 44ms/step - loss: 84.5799 - val_loss: 3.1726\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 2.7934 - val_loss: 1.5620\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.3639 - val_loss: 1.1930\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.9653 - val_loss: 0.9259\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.9211 - val_loss: 0.8140\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.7724 - val_loss: 1.6055\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.7634 - val_loss: 0.6302\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.6406 - val_loss: 0.2639\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.3092 - val_loss: 0.2888\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.3820 - val_loss: 6.1044\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 6.0210 - val_loss: 0.5603\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.5558 - val_loss: 0.4832\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4776 - val_loss: 0.5091\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4711 - val_loss: 0.5048\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4461 - val_loss: 0.4806\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4361 - val_loss: 0.4412\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.4406 - val_loss: 0.4761\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4323 - val_loss: 0.5801\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4336 - val_loss: 0.4281\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.4127 - val_loss: 0.4784\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.4125 - val_loss: 0.4146\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4128 - val_loss: 0.3672\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.4589 - val_loss: 0.4784\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3796 - val_loss: 0.3866\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2425 - val_loss: 0.2269\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2091 - val_loss: 0.2269\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2080 - val_loss: 0.2258\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.2072 - val_loss: 0.2261\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2070 - val_loss: 0.2257\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 4s 48ms/step - loss: 0.2073 - val_loss: 0.2254\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2065 - val_loss: 0.2245\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2071 - val_loss: 0.2262\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2072 - val_loss: 0.2238\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2064 - val_loss: 0.2249\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2071 - val_loss: 0.2244\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2058 - val_loss: 0.2253\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2059 - val_loss: 0.2249\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2056 - val_loss: 0.2236\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2050 - val_loss: 0.2248\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2056 - val_loss: 0.2240\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2046 - val_loss: 0.2240\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2045 - val_loss: 0.2235\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2043 - val_loss: 0.2231\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2043 - val_loss: 0.2265\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2039 - val_loss: 0.2285\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2048 - val_loss: 0.2276\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2072 - val_loss: 0.2231\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2041 - val_loss: 0.2279\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2062 - val_loss: 0.2264\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.2061 - val_loss: 0.2271\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2046 - val_loss: 0.2250\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2053 - val_loss: 0.2253\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2039 - val_loss: 0.2273\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2049 - val_loss: 0.2252\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2042 - val_loss: 0.2250\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2031 - val_loss: 0.2284\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2054 - val_loss: 0.2312\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2045 - val_loss: 0.2271\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2030 - val_loss: 0.2299\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2011 - val_loss: 0.2228\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1993 - val_loss: 0.2236\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1999 - val_loss: 0.2237\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1986 - val_loss: 0.2226\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1994 - val_loss: 0.2236\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1996 - val_loss: 0.2227\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1991 - val_loss: 0.2234\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1990 - val_loss: 0.2227\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1996 - val_loss: 0.2239\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1989 - val_loss: 0.2229\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1996 - val_loss: 0.2219\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1984 - val_loss: 0.2225\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1989 - val_loss: 0.2261\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.1993 - val_loss: 0.2229\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1988 - val_loss: 0.2248\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1994 - val_loss: 0.2256\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1992 - val_loss: 0.2231\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2001 - val_loss: 0.2245\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1989 - val_loss: 0.2256\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1999 - val_loss: 0.2233\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1985 - val_loss: 0.2221\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1985 - val_loss: 0.2249\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1990 - val_loss: 0.2225\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1991 - val_loss: 0.2236\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1989 - val_loss: 0.2257\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1997 - val_loss: 0.2233\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1981 - val_loss: 0.2244\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1985 - val_loss: 0.2234\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1974 - val_loss: 0.2232\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1968 - val_loss: 0.2226\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.1969 - val_loss: 0.2227\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 4s 53ms/step - loss: 0.1969 - val_loss: 0.2247\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.1966 - val_loss: 0.2228\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1968 - val_loss: 0.2235\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2234\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1968 - val_loss: 0.2226\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1966 - val_loss: 0.2228\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1962 - val_loss: 0.2230\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1965 - val_loss: 0.2235\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1969 - val_loss: 0.2232\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1964 - val_loss: 0.2235\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1971 - val_loss: 0.2243\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1965 - val_loss: 0.2222\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1960 - val_loss: 0.2226\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1959 - val_loss: 0.2235\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1962 - val_loss: 0.2231\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.1962 - val_loss: 0.2236\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.1961 - val_loss: 0.2232\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1955 - val_loss: 0.2232\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1963 - val_loss: 0.2237\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.1963 - val_loss: 0.2238\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.1958 - val_loss: 0.2230\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.1962 - val_loss: 0.2231\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.1966 - val_loss: 0.2231\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1962 - val_loss: 0.2235\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1961 - val_loss: 0.2234\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1963 - val_loss: 0.2236\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1959 - val_loss: 0.2234\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1959 - val_loss: 0.2236\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1959 - val_loss: 0.2232\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1954 - val_loss: 0.2232\n",
      "Fold 2 NN: 0.22191\n",
      "Training fold 3\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 43.6724 - val_loss: 2.5503\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.7262 - val_loss: 1.8289\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.5967 - val_loss: 3.5546\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 1.9115 - val_loss: 1.0108\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.1575 - val_loss: 1.9311\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 1.2073 - val_loss: 0.8389\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.6924 - val_loss: 0.6173\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.5155 - val_loss: 0.6198\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.5244 - val_loss: 0.5783\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.4911 - val_loss: 0.5811\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.5178 - val_loss: 2.6098\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 2.6747 - val_loss: 0.2395\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2540 - val_loss: 0.2233\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2377 - val_loss: 0.3020\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2402 - val_loss: 0.2160\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2288 - val_loss: 0.2166\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2216 - val_loss: 0.2270\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2347 - val_loss: 0.2255\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2478 - val_loss: 0.2370\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2332 - val_loss: 0.2358\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2440 - val_loss: 0.2289\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2316 - val_loss: 0.2204\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2331 - val_loss: 0.2189\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2371 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2443 - val_loss: 0.2128\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2415 - val_loss: 0.2376\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2357 - val_loss: 0.2312\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2441 - val_loss: 0.2188\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2382 - val_loss: 0.2397\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2452 - val_loss: 0.2751\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2389 - val_loss: 0.2410\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.2429 - val_loss: 0.2143\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2438 - val_loss: 0.2288\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2348 - val_loss: 0.2809\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.6398 - val_loss: 0.2197\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2420 - val_loss: 0.2187\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2197 - val_loss: 0.2173\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2229 - val_loss: 0.2268\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2248 - val_loss: 0.2198\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2218 - val_loss: 0.2403\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2235 - val_loss: 0.2151\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2068 - val_loss: 0.2085\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2033 - val_loss: 0.2095\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2031 - val_loss: 0.2089\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2032 - val_loss: 0.2100\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2036 - val_loss: 0.2084\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2038 - val_loss: 0.2078\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2028 - val_loss: 0.2104\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 5s 63ms/step - loss: 0.2033 - val_loss: 0.2099\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: 0.2027 - val_loss: 0.2107\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2025 - val_loss: 0.2094\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 5s 54ms/step - loss: 0.2020 - val_loss: 0.2089\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2026 - val_loss: 0.2090\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2025 - val_loss: 0.2090\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2022 - val_loss: 0.2101\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2020 - val_loss: 0.2105\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2031 - val_loss: 0.2122\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2027 - val_loss: 0.2102\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2020 - val_loss: 0.2099\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2024 - val_loss: 0.2095\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2016 - val_loss: 0.2115\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2024 - val_loss: 0.2137\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2042 - val_loss: 0.2149\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2049 - val_loss: 0.2084\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2010 - val_loss: 0.2080\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2012 - val_loss: 0.2093\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1995 - val_loss: 0.2084\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2002 - val_loss: 0.2086\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2001 - val_loss: 0.2081\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1994 - val_loss: 0.2095\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1997 - val_loss: 0.2084\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2000 - val_loss: 0.2084\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2001 - val_loss: 0.2098\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1994 - val_loss: 0.2079\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2001 - val_loss: 0.2084\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1996 - val_loss: 0.2096\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1995 - val_loss: 0.2085\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1992 - val_loss: 0.2087\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1989 - val_loss: 0.2090\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1987 - val_loss: 0.2087\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1987 - val_loss: 0.2088\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1988 - val_loss: 0.2091\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1992 - val_loss: 0.2086\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1990 - val_loss: 0.2085\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1987 - val_loss: 0.2085\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1986 - val_loss: 0.2088\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.1984 - val_loss: 0.2087\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1982 - val_loss: 0.2082\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 4s 54ms/step - loss: 0.1986 - val_loss: 0.2083\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1983 - val_loss: 0.2086\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1985 - val_loss: 0.2093\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1983 - val_loss: 0.2082\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.1983 - val_loss: 0.2088\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1982 - val_loss: 0.2086\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1981 - val_loss: 0.2086\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1977 - val_loss: 0.2090\n",
      "Fold 3 NN: 0.20781\n",
      "Training fold 4\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 69.6858 - val_loss: 3.1953\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 3.9225 - val_loss: 1.7311\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.7581 - val_loss: 0.8506\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.8298 - val_loss: 0.7218\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.7799 - val_loss: 0.9001\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.7498 - val_loss: 1.0257\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.6062 - val_loss: 0.4126\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.3450 - val_loss: 0.3551\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.3356 - val_loss: 0.3725\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3429 - val_loss: 0.3235\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3399 - val_loss: 0.3399\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.3412 - val_loss: 0.2992\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3332 - val_loss: 0.3532\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3438 - val_loss: 0.4037\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3284 - val_loss: 0.4501\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3395 - val_loss: 0.2854\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3378 - val_loss: 0.4279\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3851 - val_loss: 0.3120\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.3521 - val_loss: 1.2036\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 1.4524 - val_loss: 1.8135\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.7611 - val_loss: 0.2662\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2489 - val_loss: 0.2339\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2495 - val_loss: 0.2837\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2579 - val_loss: 0.2522\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2553 - val_loss: 0.2404\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2491 - val_loss: 0.2598\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2542 - val_loss: 0.2836\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2626 - val_loss: 0.2805\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2585 - val_loss: 0.3431\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.2614 - val_loss: 0.3291\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.2924 - val_loss: 0.2557\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2593 - val_loss: 0.2295\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2706 - val_loss: 0.3112\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2861 - val_loss: 0.3490\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2736 - val_loss: 0.2558\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2482 - val_loss: 0.5465\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3332 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2385 - val_loss: 0.2245\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3125 - val_loss: 0.2928\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2370 - val_loss: 0.2297\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2322 - val_loss: 0.2492\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2345 - val_loss: 0.2528\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2311 - val_loss: 0.2595\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2541 - val_loss: 0.2139\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2352 - val_loss: 0.2200\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2383 - val_loss: 0.2719\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3357 - val_loss: 0.4108\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3326 - val_loss: 0.2260\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2293 - val_loss: 0.2181\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2251 - val_loss: 0.2904\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2427 - val_loss: 0.2145\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2229 - val_loss: 0.2605\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2368 - val_loss: 0.2701\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2560 - val_loss: 0.2393\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2526 - val_loss: 1.0170\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.9180 - val_loss: 0.2224\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2312 - val_loss: 0.2532\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2230 - val_loss: 0.2248\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2179 - val_loss: 0.2122\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2342 - val_loss: 0.2109\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2186 - val_loss: 0.2172\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2227 - val_loss: 0.2252\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2145 - val_loss: 0.2144\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2240 - val_loss: 0.2398\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2300 - val_loss: 0.2677\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2252 - val_loss: 0.2161\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 4s 47ms/step - loss: 0.2183 - val_loss: 0.2132\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.2252 - val_loss: 0.2201\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2425 - val_loss: 0.3257\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.4907 - val_loss: 0.3808\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2517 - val_loss: 0.2266\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2257 - val_loss: 0.2166\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2197 - val_loss: 0.2128\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2154 - val_loss: 0.2171\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2235 - val_loss: 0.2104\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.2210 - val_loss: 0.2293\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2488 - val_loss: 0.2180\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2375 - val_loss: 0.2175\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2298 - val_loss: 0.2345\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2217 - val_loss: 0.2103\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2205 - val_loss: 0.2773\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2233 - val_loss: 0.2212\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 3.4255 - val_loss: 0.8834\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.6341 - val_loss: 0.5506\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5532 - val_loss: 0.5509\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.5551 - val_loss: 0.5295\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5043 - val_loss: 0.5535\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5563 - val_loss: 0.5521\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5634 - val_loss: 0.5618\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5584 - val_loss: 0.5510\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.5547 - val_loss: 0.5533\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5580 - val_loss: 0.5592\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.5563 - val_loss: 0.5515\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5588 - val_loss: 0.5558\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.5580 - val_loss: 0.5510\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.5597 - val_loss: 0.5511\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.5547 - val_loss: 0.5510\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.5523 - val_loss: 0.5511\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5539 - val_loss: 0.5510\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5547 - val_loss: 0.5510\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5530 - val_loss: 0.5516\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5536 - val_loss: 0.5530\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5573 - val_loss: 0.5532\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.5549 - val_loss: 0.5530\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.5556 - val_loss: 0.5523\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.5527 - val_loss: 0.5530\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 4s 52ms/step - loss: 0.5529 - val_loss: 0.5511\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.5534 - val_loss: 0.5517\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5555 - val_loss: 0.5510\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5561 - val_loss: 0.5530\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5543 - val_loss: 0.5518\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.5516 - val_loss: 0.5612\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5557 - val_loss: 0.5510\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5547 - val_loss: 0.5515\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5559 - val_loss: 0.5511\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5533 - val_loss: 0.5510\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5546 - val_loss: 0.5510\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5529 - val_loss: 0.5522\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5516 - val_loss: 0.5519\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5556 - val_loss: 0.5511\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.5551 - val_loss: 0.5511\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5544 - val_loss: 0.5529\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5538 - val_loss: 0.5513\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.5533 - val_loss: 0.5515\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5544 - val_loss: 0.5510\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5531 - val_loss: 0.5510\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5518 - val_loss: 0.5513\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5526 - val_loss: 0.5512\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 4s 42ms/step - loss: 0.5547 - val_loss: 0.5510\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.5539 - val_loss: 0.5510\n",
      "Fold 4 NN: 0.21029\n",
      "Training fold 5\n",
      "Epoch 1/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 51.1447 - val_loss: 2.7980\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.3459 - val_loss: 0.7157\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.6291 - val_loss: 1.4388\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 5.7352 - val_loss: 0.5295\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.5346 - val_loss: 0.2680\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3324 - val_loss: 0.2964\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.3606 - val_loss: 0.4474\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 1.9769 - val_loss: 1.8794\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.8034 - val_loss: 0.3405\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.2881 - val_loss: 0.2728\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2836 - val_loss: 0.2614\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 4s 54ms/step - loss: 0.2923 - val_loss: 0.3272\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2837 - val_loss: 0.2461\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2817 - val_loss: 0.2727\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2841 - val_loss: 0.3702\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2994 - val_loss: 0.3156\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2893 - val_loss: 0.2576\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2957 - val_loss: 0.4930\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.3438 - val_loss: 0.3083\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2937 - val_loss: 0.2274\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2897 - val_loss: 0.2893\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2838 - val_loss: 0.2420\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2675 - val_loss: 0.2623\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.2833 - val_loss: 0.2222\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2985 - val_loss: 0.2685\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2714 - val_loss: 0.2671\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2576 - val_loss: 0.2211\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2592 - val_loss: 0.2483\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2606 - val_loss: 0.2777\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2678 - val_loss: 0.2406\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2709 - val_loss: 0.2836\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2933 - val_loss: 0.4027\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 1.5407 - val_loss: 0.4558\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2774 - val_loss: 0.2145\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2333 - val_loss: 0.2147\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2228 - val_loss: 0.2165\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2232 - val_loss: 0.2148\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 5s 54ms/step - loss: 0.2185 - val_loss: 0.2149\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 6s 69ms/step - loss: 0.2188 - val_loss: 0.2318\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2172 - val_loss: 0.2123\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2167 - val_loss: 0.3671\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2703 - val_loss: 0.2159\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2313 - val_loss: 0.2136\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2311 - val_loss: 0.2121\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2218 - val_loss: 0.2307\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2251 - val_loss: 0.2243\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2193 - val_loss: 0.2218\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 4s 46ms/step - loss: 0.2243 - val_loss: 0.2124\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2268 - val_loss: 0.2097\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2307 - val_loss: 0.2174\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2193 - val_loss: 0.2136\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.3966 - val_loss: 0.3620\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2461 - val_loss: 0.2227\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2260 - val_loss: 0.2164\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2222 - val_loss: 0.2176\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2197 - val_loss: 0.2090\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2171 - val_loss: 0.2403\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2201 - val_loss: 0.2814\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.5163 - val_loss: 1.3329\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.7904 - val_loss: 0.2303\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2443 - val_loss: 0.2460\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2241 - val_loss: 0.2348\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2212 - val_loss: 0.2119\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2201 - val_loss: 0.2331\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2224 - val_loss: 0.2105\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2222 - val_loss: 0.2109\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2182 - val_loss: 0.2296\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2268 - val_loss: 0.2172\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2200 - val_loss: 0.2577\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2320 - val_loss: 0.2107\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2231 - val_loss: 0.2512\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.2278 - val_loss: 0.2321\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2098 - val_loss: 0.2078\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2038 - val_loss: 0.2071\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.2041 - val_loss: 0.2068\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2031 - val_loss: 0.2077\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2032 - val_loss: 0.2073\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2030 - val_loss: 0.2075\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2038 - val_loss: 0.2071\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2035 - val_loss: 0.2080\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2038 - val_loss: 0.2080\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2031 - val_loss: 0.2074\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2032 - val_loss: 0.2077\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2025 - val_loss: 0.2076\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2035 - val_loss: 0.2076\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2034 - val_loss: 0.2085\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.2029 - val_loss: 0.2093\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.2030 - val_loss: 0.2070\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2028 - val_loss: 0.2071\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.2020 - val_loss: 0.2074\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 4s 53ms/step - loss: 0.2025 - val_loss: 0.2102\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2011 - val_loss: 0.2064\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2012 - val_loss: 0.2068\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2009 - val_loss: 0.2066\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2015 - val_loss: 0.2076\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2010 - val_loss: 0.2078\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2007 - val_loss: 0.2079\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2010 - val_loss: 0.2067\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2003 - val_loss: 0.2069\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 4s 44ms/step - loss: 0.2006 - val_loss: 0.2069\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2009 - val_loss: 0.2084\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2010 - val_loss: 0.2077\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2012 - val_loss: 0.2080\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2006 - val_loss: 0.2071\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2002 - val_loss: 0.2077\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1995 - val_loss: 0.2087\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2009 - val_loss: 0.2084\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2001 - val_loss: 0.2074\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1998 - val_loss: 0.2075\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1999 - val_loss: 0.2076\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1999 - val_loss: 0.2078\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2006 - val_loss: 0.2076\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1999 - val_loss: 0.2077\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1996 - val_loss: 0.2075\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.2000 - val_loss: 0.2081\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1994 - val_loss: 0.2077\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1993 - val_loss: 0.2080\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1994 - val_loss: 0.2075\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2002 - val_loss: 0.2075\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.2002 - val_loss: 0.2076\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1994 - val_loss: 0.2078\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1996 - val_loss: 0.2085\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1997 - val_loss: 0.2079\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1993 - val_loss: 0.2075\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1993 - val_loss: 0.2076\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 4s 50ms/step - loss: 0.1994 - val_loss: 0.2077\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 4s 49ms/step - loss: 0.1995 - val_loss: 0.2079\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 5s 55ms/step - loss: 0.1992 - val_loss: 0.2076\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 4s 51ms/step - loss: 0.1994 - val_loss: 0.2076\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 4s 45ms/step - loss: 0.1991 - val_loss: 0.2078\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1995 - val_loss: 0.2076\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1997 - val_loss: 0.2076\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1992 - val_loss: 0.2078\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 3s 42ms/step - loss: 0.1993 - val_loss: 0.2078\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1999 - val_loss: 0.2079\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 4s 43ms/step - loss: 0.1995 - val_loss: 0.2078\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1993 - val_loss: 0.2078\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1993 - val_loss: 0.2078\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 3s 40ms/step - loss: 0.1991 - val_loss: 0.2077\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1994 - val_loss: 0.2078\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 3s 41ms/step - loss: 0.1996 - val_loss: 0.2077\n",
      "Fold 5 NN: 0.2064\n",
      "NN folds RMSPE is: 0.2122215370361088\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "val_predictions_nn_1 = copy.copy(train_nn[['stock_id', 'time_id', 'target']])\n",
    "val_predictions_nn_1['predictions'] = 0.\n",
    "test_predictions_nn_1 = copy.copy(test_nn[['stock_id', 'time_id']])\n",
    "val_nn_1, test_nn_1 = train_and_evaluate_nn(train_nn, test_nn, nn_params1, True)\n",
    "val_predictions_nn_1['predictions'] = val_nn_1\n",
    "test_predictions_nn_1['target'] = test_nn_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aebbac39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T07:35:57.831595Z",
     "iopub.status.busy": "2021-09-27T07:35:57.830904Z",
     "iopub.status.idle": "2021-09-27T07:35:57.842406Z",
     "shell.execute_reply": "2021-09-27T07:35:57.841790Z"
    },
    "papermill": {
     "duration": 11.129079,
     "end_time": "2021-09-27T07:35:57.842539",
     "exception": false,
     "start_time": "2021-09-27T07:35:46.713460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN CV merged: 0.2110056647065544\n"
     ]
    }
   ],
   "source": [
    "merged_predictions_nn = 0.5 * (val_predictions_nn_0['predictions'] + val_predictions_nn_1['predictions'])\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)\n",
    "print(\"NN CV merged: {}\".format(rmspe(val_predictions_nn_0['target'], merged_predictions_nn)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5a10c06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T07:36:20.357529Z",
     "iopub.status.busy": "2021-09-27T07:36:20.356924Z",
     "iopub.status.idle": "2021-09-27T07:36:20.369470Z",
     "shell.execute_reply": "2021-09-27T07:36:20.370106Z"
    },
    "papermill": {
     "duration": 11.328256,
     "end_time": "2021-09-27T07:36:20.370334",
     "exception": false,
     "start_time": "2021-09-27T07:36:09.042078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN CV merged and clipped: 0.21099788966822008\n"
     ]
    }
   ],
   "source": [
    "clipped_predictions_nn = np.clip(merged_predictions_nn, 1e-4, 0.072)\n",
    "print(\"NN CV merged and clipped: {}\".format(rmspe(val_predictions_nn_0['target'], clipped_predictions_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d580cd83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T07:36:42.475041Z",
     "iopub.status.busy": "2021-09-27T07:36:42.474453Z",
     "iopub.status.idle": "2021-09-27T07:36:42.486758Z",
     "shell.execute_reply": "2021-09-27T07:36:42.486166Z"
    },
    "papermill": {
     "duration": 11.153862,
     "end_time": "2021-09-27T07:36:42.486908",
     "exception": false,
     "start_time": "2021-09-27T07:36:31.333046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm + NN CV merged: 0.21396948672978533\n"
     ]
    }
   ],
   "source": [
    "merged_predictions_all = 0.5 * (merged_predictions + merged_predictions_nn)\n",
    "print(\"lgbm + NN CV merged: {}\".format(rmspe(val_predictions_nn_0['target'], merged_predictions_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb98b2c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T07:37:04.669513Z",
     "iopub.status.busy": "2021-09-27T07:37:04.668190Z",
     "iopub.status.idle": "2021-09-27T07:37:04.681488Z",
     "shell.execute_reply": "2021-09-27T07:37:04.680997Z"
    },
    "papermill": {
     "duration": 10.965097,
     "end_time": "2021-09-27T07:37:04.681620",
     "exception": false,
     "start_time": "2021-09-27T07:36:53.716523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgbm + NN CV merged and clipped: 0.21396948672978533\n"
     ]
    }
   ],
   "source": [
    "clipped_predictions_all = np.clip(merged_predictions_all, 1e-4, 0.072)\n",
    "print(\"lgbm + NN CV merged and clipped: {}\".format(rmspe(val_predictions_nn_0['target'], clipped_predictions_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94d632ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T07:37:26.933654Z",
     "iopub.status.busy": "2021-09-27T07:37:26.932945Z",
     "iopub.status.idle": "2021-09-27T07:37:26.935959Z",
     "shell.execute_reply": "2021-09-27T07:37:26.936388Z"
    },
    "papermill": {
     "duration": 11.2326,
     "end_time": "2021-09-27T07:37:26.936546",
     "exception": false,
     "start_time": "2021-09-27T07:37:15.703946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['target'] = 0.5 * (0.5 * (test_predictions_lgb_0['target']+ test_predictions_lgb_1['target'])+\n",
    "                        0.5 * (test_predictions_nn_0['target'] + test_predictions_nn_1['target']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7491.216107,
   "end_time": "2021-09-27T07:38:03.491439",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T05:33:12.275332",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
